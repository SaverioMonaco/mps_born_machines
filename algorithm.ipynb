{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a49cc9-4962-4046-b604-41f2b2d66695",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Born Machine through MPS\n",
    "## Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ae765-f6a7-46c4-92ed-001dbf4d0655",
   "metadata": {},
   "source": [
    "#### Legend:\n",
    "* <font color='blue'>blue</font> means there is still some doubts about the procedure.\n",
    "* <font color='green'>green</font> means there are some details further discussed about the argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce81562-d63e-48bd-80df-073bce6686c8",
   "metadata": {},
   "source": [
    "***Goal :*** Obtain a wavefunction ${\\psi}$ expressed through a MPS Network so that its probability distribution\n",
    "$$ P(v) = \\frac{|\\psi(v)|^2}{Z}; \\qquad Z = \\sum_{v\\in V} |\\psi(v)|^2 $$\n",
    "Resembles the latent probability distribution of the data it is trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36077c40-1809-4f9d-8343-91ab15b03c49",
   "metadata": {},
   "source": [
    "The above task is done by minimizing the Negative Log-Likelihood.\\\n",
    "<font color='green'>Minimizing the NLL is equivalent of minimizing the Kullback-Leibler Divergence.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd9bc0-d083-4018-a0fd-ddc5d924a15c",
   "metadata": {},
   "source": [
    "1. ***Initialize*** MPS <font color='green'>randomly</font>;\n",
    "\n",
    "2. ***<font color='green'>Canonicalize</font>*** the Tensor Network;\n",
    "\n",
    "3. At each step:\n",
    "\n",
    "    ***Compute the derivative***\n",
    "    \n",
    "    3.1 <font color='green'>Merge two adiacents tensors</font> into a rank-4 tensor:\n",
    "    \n",
    "    <img src=\"./imgs/algorithm_merge.svg\">\n",
    "    \n",
    "    ***Update Network***\n",
    "    \n",
    "    3.2 <font color='blue'>Update the merged tensor</font>  $A^{k,k+1}$\n",
    "    \n",
    "    3.3 Unfold the merged rank-4 tensor through SVD, <font color='green'>keeping mixed canonicalization of the network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e31a1-cf6e-4a71-bec3-12260f0f3c9e",
   "metadata": {},
   "source": [
    "***Generation of Samples***\n",
    "\n",
    "4. <font color='blue'>idk</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540a84b-a99d-451f-b049-89086021c551",
   "metadata": {},
   "source": [
    "***Additional Tasks***\n",
    "\n",
    "5. <font color='blue'>Reconstruction task</font>\n",
    "\n",
    "6. <font color='blue'>Can we denoise images?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a2c13-2424-4439-9261-759ee2c73e6f",
   "metadata": {},
   "source": [
    "### <font color='blue'>Still Unanswered</font>\n",
    "\n",
    "##### ***3.2 How does the update work exactly?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1c5f9-7015-4c9f-b471-97e15d1caaaf",
   "metadata": {},
   "source": [
    "### <font color='green'>Further Details</font>\n",
    "\n",
    "##### ***1 Do we need to apply some sort of normalization?***\n",
    "Most probably it does not matter and updating the TN will change normalization of the network anyway. Although maybe the normalization of the Tensor Network has the same impact of _Weights initialization_ in Neural Networks\n",
    "\n",
    "##### ***2 Canonicalization***\n",
    "See canonicalization.ipynb\n",
    "\n",
    "##### ***3.1 Why should we do the merge?***\n",
    "(From main paper, last paragraph of page 12)\n",
    "\n",
    "Most probably to keep the network canonicalized after the updates in learning in a smart way, let me explain:\n",
    "* MPS is mixed canonicalize around $A^{k}$\n",
    "* Merge tensors $A^{k}$ and $A^{k+1}$ into a Rank-4 Tensor $A^{k,k+1}$\n",
    "\n",
    "<img src=\"./imgs/algorithm_mixedca.svg\">\n",
    "\n",
    "* Update components of tensor $A^{k,k+1}$\n",
    "\n",
    "* Unfold $A^{k,k+1}$ so that $A^k = S$ and $A^{k+1}=VD$ in SVD algorithm\n",
    "\n",
    "* Merge $A^{k+1}$ with $A^{k+2}$, MPS network is now canonicalized around the tensor we want to do the update on\n",
    "\n",
    "##### ***3.3 After updating and unfolding, one of the two tensors is probably not orthogonal anymore, should we apply canonicalization again?***\n",
    "Yes, See <font color='green'>3.1</font>\n",
    "\n",
    "##### ***Minimization of KL-Divergence***\n",
    "Suppose:\n",
    "\n",
    "$P(x|\\vartheta^*)$ being the true distribution (that we want to learn)\n",
    "\n",
    "$P(x|\\vartheta)$ being our estimate\n",
    "\n",
    "By the definition of Kullback-Leibler Divergence:\n",
    "$$D_{KL}\\left[P(x|\\vartheta^*)||P(x|\\vartheta)\\right] := E_{x\\sim P(x|\\vartheta^*)}\\left[\\frac{P(x|\\vartheta^*)}{P(x|\\vartheta)}\\right]$$\n",
    "Applying the properties of logarithms:\n",
    "$$E_{x\\sim P(x|\\vartheta^*)}\\left[\\frac{P(x|\\vartheta^*)}{P(x|\\vartheta)}\\right] = E_{x\\sim P(x|\\vartheta^*)}\\left[{P(x|\\vartheta^*)}-{P(x|\\vartheta)}\\right]$$\n",
    "Applying the property of the expected value function:\n",
    "$$E_{x\\sim P(x|\\vartheta^*)}\\left[{P(x|\\vartheta^*)}-{P(x|\\vartheta)}\\right] = E_{x\\sim P(x|\\vartheta^*)}\\left[P(x|\\vartheta^*)\\right] - E_{x\\sim P(x|\\vartheta)}\\left[P(x|\\vartheta^*)\\right]$$\n",
    "Hence:\n",
    "$$D_{KL}\\left[P(x|\\vartheta^*)||P(x|\\vartheta)\\right] = E_{x\\sim P(x|\\vartheta^*)}\\left[P(x|\\vartheta^*)\\right] - E_{x\\sim P(x|\\vartheta)}\\left[P(x|\\vartheta^*)\\right]$$\n",
    "\n",
    "Considering just the second term\n",
    "$$ - E_{x\\sim P(x|\\vartheta)}\\left[P(x|\\vartheta^*)\\right] \\approx -\\frac{1}{N}\\sum_{i}^{N}\\log P(x_i|\\vartheta)  \\propto \\mathcal{L} $$\n",
    "\n",
    "Hence: \n",
    "$$D_{KL}\\left[P(x|\\vartheta^*)||P(x|\\vartheta)\\right] = E_{x\\sim P(x|\\vartheta^*)}\\left[P(x|\\vartheta^*)\\right] - \\mathcal{L}(\\vartheta)$$\n",
    "\n",
    "\n",
    "(Ez, we can reference also the [paper](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full) from Kullback and Leibler to be fancy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
