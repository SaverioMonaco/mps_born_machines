{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8562bae-7049-485e-8a4f-a80d234aa2e9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e99b1f-d534-495f-a9d5-9a7475aef6e5",
   "metadata": {},
   "source": [
    "Some functions that i wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189971dc-b444-49a8-9b73-41acbb4ef7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### IMPORTS ####\n",
    "#################\n",
    "\n",
    "# Profiling\n",
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "\n",
    "# Arrays\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning stuff\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Images display and plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fancy progress bars\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "# Tensor Network Stuff\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import quimb.tensor as qtn # Tensor Network library\n",
    "import quimb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ae909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_profiler(func):\n",
    "    '''Generic profiler. Expects an argument-free function.\n",
    "    e. g. func = lambda: learning_epoch_SGD(mps, imgs, 3, 0.1).\n",
    "    Prints and returns the profiling report trace.'''\n",
    "    # TODO: adapt to write trace to file\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    func()\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    sortby = SortKey.CUMULATIVE\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats()\n",
    "    print(s.getvalue())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1842c546-1c87-4e5b-853b-db3f428063ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Wrapper for type checks.\n",
    "While defining a function, you can add the wrapper\n",
    "stating the expected types:\n",
    "> @arg_val(class_1, class_2, ...)\n",
    "> def function(a, b, ...): \n",
    "'''\n",
    "def arg_val(*args):\n",
    "    def wrapper(func):\n",
    "        def validating(*_args):\n",
    "            if any(type(_arg)!=arg for _arg, arg in zip(_args,args)):\n",
    "                raise TypeError('wrong type!')\n",
    "            return func(*_args)\n",
    "        return validating\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d2ec9f-a2e7-4e27-9e74-24bdfc4a296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@arg_val(int, int, float)\n",
    "def get_data(train_size = 1000, test_size = 100, grayscale_threshold = .5):\n",
    "    '''\n",
    "    Prepare the MNIST dataset for the training algorithm:\n",
    "     * Choose randomly a subset from the whole dataset\n",
    "     * Flatten each image to mirror the mps structure\n",
    "     * Normalize images from [0,255] to [0,1]\n",
    "     * Apply a threshold for each pixels so that each value \n",
    "       below that threshold are set to 0, the others get set to 1.\n",
    "       For this algorithm we will only deal to binary states {0,1}\n",
    "       instead of a range from 0 to 1    \n",
    "    '''\n",
    "    # Download all data\n",
    "    mnist = torchvision.datasets.MNIST('classifier_data', train=True, download=True,\n",
    "                                                  transform = transforms.Compose([transforms.ToTensor()]) )\n",
    "    \n",
    "    # Convert torch.tenor to numpy\n",
    "    npmnist = mnist.data.numpy()\n",
    "    \n",
    "    # Check of the type of the sizes\n",
    "    #if ((type(train_size) != int) or (type(test_size) != int)):\n",
    "    #    raise TypeError('train_size and test_size must be INT')\n",
    "    \n",
    "    # Check if the training_size and test_size requested are bigger than\n",
    "    # the MNIST whole size\n",
    "    if ( (train_size + test_size) > npmnist.shape[0] ):\n",
    "        raise ValueError('Subset too big')\n",
    "    \n",
    "    # Check of the positivity of sizes\n",
    "    if ( (train_size <= 0) or (test_size <= 0) ):\n",
    "        raise ValueError('Size of training set and test set cannot be negative')\n",
    "    \n",
    "    # Choose just a subset of the data\n",
    "    # Creating a mask by randomly sampling the indexes of the full dataset\n",
    "    subset_indexes = np.random.choice(np.arange(npmnist.shape[0]), size=(train_size + test_size), \n",
    "                                      replace=False, p=None)\n",
    "    \n",
    "    # Apply the mask\n",
    "    npmnist = npmnist[subset_indexes]\n",
    "    \n",
    "    # Flatten every image\n",
    "    npmnist = np.reshape(npmnist, (npmnist.shape[0], npmnist.shape[1]*npmnist.shape[2]))\n",
    "    \n",
    "    # Normalize the data from 0 - 255 to 0 - 1\n",
    "    npmnist = npmnist/npmnist.max()\n",
    "    \n",
    "    # As in the paper, we will only deal with {0,1} values, not a range\n",
    "    \n",
    "    if ((grayscale_threshold <= 0) or (grayscale_threshold >= 1)):\n",
    "        raise ValueError('grayscale_threshold must be in range ]0,1[')\n",
    "    \n",
    "    npmnist[npmnist > grayscale_threshold] = 1\n",
    "    npmnist[npmnist <= grayscale_threshold] = 0\n",
    "    \n",
    "    # Return training set and test set\n",
    "    return npmnist[:train_size], npmnist[train_size:]\n",
    "\n",
    "@arg_val(np.ndarray, bool, str)\n",
    "def plot_img(img_flat, flip_color = True, savefig = ''):\n",
    "    '''\n",
    "    Display the image from the flattened form\n",
    "    '''\n",
    "    # If the image is corrupted for partial reconstruction (pixels are set to -1)\n",
    "    if -1 in img_flat:\n",
    "        img_flat = np.copy(img_flat)\n",
    "        img_flat[img_flat == -1] = 0\n",
    "    \n",
    "    # Background white, strokes black\n",
    "    if flip_color:\n",
    "        plt.imshow(1-np.reshape(img_flat,(28,28)), cmap='gray')\n",
    "    # Background black, strokes white\n",
    "    else:\n",
    "        plt.imshow(np.reshape(img_flat,(28,28)), cmap='gray')\n",
    "        \n",
    "    plt.axis('off')\n",
    "    \n",
    "    if savefig != '':\n",
    "        # save the picture as svg in the location determined by savefig\n",
    "        plt.savefig(savefig, format='svg')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fe208f-e019-4718-b55f-6af2ef3d4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c308e2-b5da-4d51-8ee9-a30319b82824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2c0833-3b8d-4259-b095-4fa23c6c7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28169cc1-f139-4e58-a553-8ac7ab78db94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ef578f-6c8f-4505-90c0-77b1c283f23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADUUlEQVR4nO3dwWoCMRRA0ab4/7+crroolQ5UM3M15ywVhri4PPARHXPOD6Dn8+oDAPeJE6LECVHihChxQtTt4H1f5cJ6496LJidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDgh6nb1AXiuMcayZ885lz2b30xOiBInRIkTosQJUeKEKHFClFXKBVauO1Y6OrdVy3OZnBAlTogSJ0SJE6LECVHihChxQpQ95wKvusd8lD3oc5mcECVOiBInRIkTosQJUeKEKHFClD3nP+y6x+RcJidEiROixAlR4oQocUKUOCFKnBBlz3nHlXvM1Xce7Whfh8kJUeKEKHFClDghSpwQJU6IEidEbbnnLO/6ymfjXCYnRIkTosQJUeKEKHFClDghastVymqPXPuySuGbyQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRLnPucCudzJX/33hbkxOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFqy/ucR/cOX/k+5jt/tt2YnBAlTogSJ0SJE6LECVHihKgtVylHHl1H+IlInsHkhChxQpQ4IUqcECVOiBInRIkTouw5/8EekzOYnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqNvVB+B9jDH+fH/OedJJ3oPJCVHihChxQpQ4IUqcECVOiBInRNlzbuZo13i0q+Q8JidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IcqVMX7w85UdJidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEHV0n9P/wcFFTE6IEidEiROixAlR4oQocULUF+EaL/VbRhgOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(train_set[1], True, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabba680-6a17-45b6-928d-fef818afa679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d72f97-9618-48a8-88c9-6b719de50b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MPS network randomly initialized\n",
    "mps = qtn.MPS_rand_state(L=28*28, bond_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43529da-7ed8-48bc-9273-1332406ef0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_img(mps, img):\n",
    "    '''\n",
    "    Contract the MPS network with an image to compute its probability\n",
    "    P(img) = (<mps|img><img|mps>)/<mps|mps>\n",
    "    '''\n",
    "    if (len(mps.tensors) != img.shape[0]):\n",
    "        raise ValueError('Length of MPS and size of image do not match')\n",
    "    \n",
    "    # Compute denominator\n",
    "    Z = mps.H @ mps # Does it acknowledge canonicalization to speed computations?\n",
    "                    # TO DO: check documentation\n",
    "        \n",
    "    # Contract image with mps\n",
    "    P = 0\n",
    "    # From left to right...\n",
    "    for body in range(img.shape[0]):\n",
    "        # if pixel is 0:\n",
    "        if img[body] == 0:\n",
    "            state = [1,0]\n",
    "        # if pixel is 1:\n",
    "        elif img[body] == 1:\n",
    "            state = [0,1]\n",
    "        else:\n",
    "            raise ValueError('Found invalid pixel in image')\n",
    "        \n",
    "        if body == img.shape[0] - 1:\n",
    "            newtensor = np.einsum('i,ik', carried_value, mps.tensors[body].data)\n",
    "            P = np.einsum('i,i', state, newtensor)\n",
    "        elif body > 0:\n",
    "            newtensor = np.einsum('i,ikj', carried_value, mps.tensors[body].data)\n",
    "            carried_value = np.einsum('i,ik', state, newtensor)\n",
    "        else:\n",
    "            carried_value = np.einsum('i,ki', state, mps.tensors[body].data)\n",
    "        \n",
    "        P = (P*P)/Z\n",
    "        \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeadb420-0933-4547-9bbb-3d37ffecbf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab247c4-1f09-4f7e-954c-65b117918133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_removal_img(mnistimg, fraction = .5, axis = 0):\n",
    "    '''\n",
    "    Corrupt (with -1 values) a portion of an input image (from the test set)\n",
    "    to test if the algorithm can reconstruct it\n",
    "    '''\n",
    "    # Check type:\n",
    "    if [type(mnistimg), type(fraction), type(axis)] != [np.ndarray, float, int]:\n",
    "        raise TypeError('Input types not valid')\n",
    "    \n",
    "    # Check the shape of input image\n",
    "    if (mnistimg.shape[0] != 784):\n",
    "        raise TypeError('Input image shape does not match, need (784,)')\n",
    "    \n",
    "    # Axis can be either 0 (rowise deletion) or 1 (columnwise deletion)\n",
    "    if not(axis in [0,1]):\n",
    "        raise ValueError('Invalid axis [0,1]')\n",
    "    \n",
    "    # Fraction must be from 0 to 1\n",
    "    if (fraction < 0 or fraction > 1):\n",
    "        raise ValueError('Invalid value for fraction variable (in interval [0,1])')\n",
    "        \n",
    "    mnistimg_corr = np.copy(mnistimg)\n",
    "    mnistimg_corr = np.reshape(mnistimg_corr, (28,28))\n",
    "    \n",
    "    if axis == 0:\n",
    "        mnistimg_corr[int(28*(1-fraction)):,:] = -1\n",
    "    else:\n",
    "        mnistimg_corr[:,int(28*(1-fraction)):] = -1\n",
    "        \n",
    "    mnistimg_corr = np.reshape(mnistimg_corr, (784,))\n",
    "    \n",
    "    return mnistimg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88236d8-ecb1-4409-ae39-b5b4035177b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = partial_removal_img(test_set[0], fraction = .3, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999e1270-4e9d-43c6-b749-659c3105077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-03-14T21:11:12.014964</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 231.84 \n",
       "L 231.84 231.84 \n",
       "L 231.84 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p705a02f042)\">\n",
       "    <image height=\"218\" id=\"image82318b7ddd\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAADCElEQVR4nO3d20rDQBRAUSP+/y+Pz0Kb0NbZua31LsTI5sAcJi5jjPEFTPW99wPAHQgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAj87P0AsyzL8tHPjzH+6Unu5dP3vubMfxMTDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQIHDZ+2jsY+vO2Mz7akdmokFAaBAQGgSEBgGhQUBoEHC8/8TWMfSZP302012P77eYaBAQGgSEBgGhQUBoEBAaBIQGgcvu0VzX4EhMNAgIDQJCg4DQICA0CAgNAkKDwGX3aLO5r8YrTDQICA0CQoOA0CAgNAgIDQJCg8AyLHwemnlf7cyv3Ht5j4kGAaFBQGgQEBoEhAYBoUFAaBBwH40/fO9yDhMNAkKDgNAgIDQICA0CQoOA4/0nZv7bpzt/qu7Kv9saEw0CQoOA0CAgNAgIDQJCg4DQIGCP9iZ7Nl5hokFAaBAQGgSEBgGhQUBoEBAaBOzRTsgn4c7HRIOA0CAgNAgIDQJCg4DQICA0CNijTbJ2Z+zKezB35R4z0SAgNAgIDQJCg4DQICA0CAgNAvZoO5j5TUiOyUSDgNAgIDQICA0CQoOA0CDgeP+Ajnz87xrMe0w0CAgNAkKDgNAgIDQICA0CQoOAPdoJzdyz2ZPNYaJBQGgQEBoEhAYBoUFAaBAQGgTs0S7ILux4TDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQK/xK1Iwas77JcAAAAASUVORK5CYII=\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p705a02f042\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a800c2-927f-438f-acf5-aff534c63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mps(Ldim = 28*28, bdim = 30, canonicalize = 1):\n",
    "    '''\n",
    "    Initialize the MPS tensor network\n",
    "    1. Create the MPS TN\n",
    "    2. Canonicalization\n",
    "    3. Renaming indexes\n",
    "    '''\n",
    "    # Create a simple MPS network randomly initialized\n",
    "    mps = qtn.MPS_rand_state(L=Ldim, bond_dim=bdim)\n",
    "    \n",
    "    # Canonicalize: use a canonicalize value out of range to skip it (such as -1)\n",
    "    if canonicalize in range(Ldim):\n",
    "        mps.canonize(canonicalize)\n",
    "        \n",
    "    # REINDEXING TENSORS FOR A EASIER DEVELOPING\n",
    "    # during initializations, the index will be named using the same notation of the \n",
    "    # Pan Zhang et al paper:\n",
    "    #  ___       ___                      ___\n",
    "    # |I0|--i0--|I1|--i1-... ...-i(N-1)--|IN|\n",
    "    #  |         |                        |\n",
    "    #  | v0      | v1                     | vN\n",
    "    #  V         V                        V\n",
    "    \n",
    "    # Reindexing the leftmost tensor\n",
    "    mps = mps.reindex({mps.tensors[0].inds[0]: 'i0', \n",
    "                       mps.tensors[0].inds[1]: 'v0'})\n",
    "    \n",
    "    # Reindexing the inner tensors through a cycle\n",
    "    for tensor in range(1,len(mps.tensors)-1):\n",
    "        mps = mps.reindex({mps.tensors[tensor].inds[0]: 'i'+str(tensor-1),\n",
    "                           mps.tensors[tensor].inds[1]: 'i'+str(tensor),\n",
    "                           mps.tensors[tensor].inds[2]: 'v'+str(tensor)})\n",
    "    \n",
    "    # Reindexing the last tensor\n",
    "    tensor = tensor + 1\n",
    "    mps = mps.reindex({mps.tensors[tensor].inds[0]: 'i'+str(tensor), \n",
    "                       mps.tensors[tensor].inds[1]: 'v'+str(tensor)})  \n",
    "    \n",
    "    return mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db50fa1-3e86-42b2-b7ee-e75e30a3d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = initialize_mps(Ldim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a501bb46-7f23-4d50-ba12-af998ccedaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=(2, 2), inds=('i0', 'v0'), tags=oset(['I0'])),\n",
       " Tensor(shape=(2, 8, 2), inds=('i0', 'i1', 'v1'), tags=oset(['I1'])),\n",
       " Tensor(shape=(8, 4, 2), inds=('i1', 'i2', 'v2'), tags=oset(['I2'])),\n",
       " Tensor(shape=(4, 2, 2), inds=('i2', 'i4', 'v3'), tags=oset(['I3'])),\n",
       " Tensor(shape=(2, 2), inds=('i4', 'v4'), tags=oset(['I4'])))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b443abba-4337-4e25-b778-58b29eda2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quimb_transform_img2state(img):\n",
    "    '''\n",
    "    Trasform an image to a tensor network to fully manipulate\n",
    "    it using quimb, may be very slow, use it for checks\n",
    "    '''\n",
    "    \n",
    "    # Initialize empty tensor\n",
    "    img_TN = qtn.Tensor()\n",
    "    for k, pixel in enumerate(img):\n",
    "        if pixel == 0: # if pixel is 0, we want to have a tensor with data [0,1]\n",
    "            img_TN = img_TN &  qtn.Tensor(data=[0,1], inds=['v'+str(k)], )\n",
    "            \n",
    "        else: # if pixel is 1, we want to have a tensor with data [1,0]\n",
    "            img_TN = img_TN &  qtn.Tensor(data=[1,0], inds=['v'+str(k)], )\n",
    "     \n",
    "    # |  | 781 |\n",
    "    # O  O ... O\n",
    "    return img_TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f6cbcdb-9de6-4355-af95-419de61659a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computepsi(mps, img):\n",
    "    '''\n",
    "    Contract the MPS with the states (pixels) of a binary{0,1} image\n",
    "    \n",
    "    PSI:    O-...-O-O-O-...-O\n",
    "            |     | | |     |\n",
    "            |     | | |     |\n",
    "    IMAGE:  O     O O O     O\n",
    "    \n",
    "    Images state are created the following way:\n",
    "    if pixel is 0 -> state = [0,1]\n",
    "    if pixel is 1 -> state = [1,0]\n",
    "    '''\n",
    "    \n",
    "    # Left most tensor\n",
    "    #          O--\n",
    "    # Compute  |  => O--\n",
    "    #          O\n",
    "    if img[0] == 0:\n",
    "        contraction = np.einsum('a,ba',[0,1], mps.tensors[0].data)\n",
    "    else:\n",
    "        contraction = np.einsum('a,ba',[1,0], mps.tensors[0].data)\n",
    "        \n",
    "    # Remove the first and last pixels because in the MPS\n",
    "    # They need to be treated differently\n",
    "    for k, pixel in enumerate(img[1:-1]):\n",
    "        #  \n",
    "        # Compute  O--O--  => O--\n",
    "        #             |       |\n",
    "        contraction = np.einsum('a,abc',contraction, mps.tensors[k+1].data)\n",
    "        \n",
    "        #          O--\n",
    "        # Compute  |  => O--\n",
    "        #          O        \n",
    "        if pixel == 0:\n",
    "            contraction = np.einsum('a,ba', [0,1], contraction)\n",
    "        else:\n",
    "            contraction = np.einsum('a,ba', [1,0], contraction)\n",
    "    \n",
    "    #          \n",
    "    # Compute  O--O  => O\n",
    "    #             |     |\n",
    "    contraction = np.einsum('a,ab',contraction, mps.tensors[-1].data)\n",
    "    \n",
    "    #          O\n",
    "    # Compute  |  => O (SCALAR)\n",
    "    #          O     \n",
    "    if img[-1] == 0:\n",
    "        contraction = np.einsum('a,a', [0,1], contraction)\n",
    "    else:\n",
    "        contraction = np.einsum('a,a', [1,0], contraction)\n",
    "    \n",
    "    return contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36765976-6972-460d-bab0-adcf2cf34331",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = initialize_mps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e69d4e2d-f36c-4d47-8024-957ec44f1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 ms, sys: 146 µs, total: 11.9 ms\n",
      "Wall time: 10.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "fast_psi = computepsi(mps, train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "911c1264-e92c-42a5-9b7a-b0c1a348f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.71 s, sys: 3.83 ms, total: 2.72 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "slow_psi = quimb_transform_img2state(train_set[0]) @ mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c71d7d-ba5d-4ddb-b57b-7dcb3f578568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.043724773601554e-252"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_psi**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda11c23-fce1-45a1-a83d-fe61e1aa72ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.043724773599092e-252"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_psi**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf51c02c-1582-4fb2-a404-0b9a9a7e2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computepsiprime(mps, img, contracted_left_index):\n",
    "    '''\n",
    "    Contract the MPS with the states (pixels) of a binary{0,1} image\n",
    "    \n",
    "    PSI':    O-...-O-      -O-...-O\n",
    "             |     |        |     |\n",
    "             |     |  |  |  |     |\n",
    "    IMAGE:   O     O  O  O  O     O\n",
    "    \n",
    "    Images state are created the following way:\n",
    "    if pixel is 0 -> state = [0,1]\n",
    "    if pixel is 1 -> state = [1,0]\n",
    "    '''\n",
    "    \n",
    "    #############\n",
    "    # LEFT PART #\n",
    "    #############\n",
    "    \n",
    "    # Left most tensor\n",
    "    #          O--\n",
    "    # Compute  |  => O--\n",
    "    #          O\n",
    "    if img[0] == 0:\n",
    "        contraction_sx = np.einsum('a,ba',[0,1], mps.tensors[0].data)\n",
    "    else:\n",
    "        contraction_sx = np.einsum('a,ba',[1,0], mps.tensors[0].data)\n",
    "        \n",
    "    for k in range(1, contracted_left_index):\n",
    "        #  \n",
    "        # Compute  O--O--  => O--\n",
    "        #             |       |\n",
    "        contraction_sx = np.einsum('a,abc->bc',contraction_sx, mps.tensors[k].data)\n",
    "        \n",
    "        #          O--\n",
    "        # Compute  |  => O--\n",
    "        #          O        \n",
    "        if img[k] == 0:\n",
    "            contraction_sx = np.einsum('a,ba', [0,1], contraction_sx)\n",
    "        else:\n",
    "            contraction_sx = np.einsum('a,ba', [1,0], contraction_sx)\n",
    "    \n",
    "    ##############\n",
    "    # RIGHT PART #\n",
    "    ##############\n",
    "    \n",
    "    # Right most tensor\n",
    "    #          ---O\n",
    "    # Compute     |  => --O\n",
    "    #             O\n",
    "    if img[-1] == 0:\n",
    "        contraction_dx = np.einsum('a,ba',[0,1], mps.tensors[-1].data)\n",
    "    else:\n",
    "        contraction_dx = np.einsum('a,ba',[1,0], mps.tensors[-1].data)\n",
    "        \n",
    "    for k in range(len(mps.tensors)-2, contracted_left_index+1, -1):\n",
    "        #  \n",
    "        # Compute  --O--O  => --O\n",
    "        #               |       |\n",
    "        \n",
    "        contraction_dx = np.einsum('a,bac->bc',contraction_dx, mps.tensors[k].data)\n",
    "        \n",
    "        #          --O\n",
    "        # Compute    |  => --O\n",
    "        #            O        \n",
    "        if img[k] == 0:\n",
    "            contraction_dx = np.einsum('a,ba', [0,1], contraction_dx)\n",
    "        else:\n",
    "            contraction_dx = np.einsum('a,ba', [1,0], contraction_dx)\n",
    "    \n",
    "    # From here on it is just speculation\n",
    "    \n",
    "    if img[contracted_left_index] == 0:\n",
    "        contraction_sx = np.einsum('a,k->ak', contraction_sx, [0,1])\n",
    "    else:\n",
    "        contraction_sx = np.einsum('a,k->ak', contraction_sx, [1,0])\n",
    "        \n",
    "    if img[contracted_left_index+1] == 0:\n",
    "        contraction_dx = np.einsum('a,k->ak', contraction_dx, [0,1])\n",
    "    else:\n",
    "        contraction_dx = np.einsum('a,k->ak', contraction_dx, [1,0])\n",
    "    \n",
    "    contraction = np.einsum('ab,cd->abcd', contraction_sx, contraction_dx)\n",
    "    \n",
    "    return contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7d461-905d-4430-ad30-13f6e6defb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_step(mps, index, imgs, lr, going_right = True):\n",
    "    '''\n",
    "    Compute the updated merged tensor A_{index,index+1}\n",
    "    \n",
    "      UPDATE RULE:  A_{i,i+1} += lr* 2 *( A_{i,i+1}/Z - ( SUM_{i=1}^{m} psi'(v)/psi(v) )/m )\n",
    "      \n",
    "    '''\n",
    "    Z = mps @ mps \n",
    "    \n",
    "    # Merge I_k and I_{k+1} in a single rank 4 tensor ('i_{k-1}', 'v_k', 'i_{k+1}', 'v_{k+1}')\n",
    "    A = (mps.tensors[index] @ mps.tensors[index+1])\n",
    "    \n",
    "    # Computing the second term, summation over\n",
    "    # the data-dependent terms\n",
    "    psifrac = 0\n",
    "    for img in imgs:\n",
    "        num = computepsiprime(mps,img,index)  # PSI(v)\n",
    "        den = computepsi(mps,img)             # PSI(v')\n",
    "        \n",
    "        # Theoretically the two computations above can be optimized in a single function\n",
    "        # because we are contracting the very same tensors for the most part\n",
    "        \n",
    "        psifrac = psifrac + num/den\n",
    "    \n",
    "    psifrac = psifrac/imgs.shape[0]\n",
    "    \n",
    "    # Derivative of the NLL\n",
    "    dNLL = (A/Z) - psifrac\n",
    "    \n",
    "    A = A + lr*dNLL # Update A_{i,i+1}\n",
    "    \n",
    "    # Now the tensor A_{i,i+1} must be split in I_k and I_{k+1}.\n",
    "    # To preserve canonicalization:\n",
    "    # > if we are merging sliding towards the RIGHT we need to absorb right\n",
    "    #                                           S  v  D\n",
    "    #     ->-->--A_{k,k+1}--<--<-   =>   ->-->-->--x--<--<--<-   =>    >-->-->--o--<--<-  \n",
    "    #      |  |    |   |    |  |          |  |  |   |    |  |          |  |  |  |  |  |\n",
    "    #\n",
    "    # > if we are merging sliding toward the LEFT we need to absorb left\n",
    "    #\n",
    "    if going_right:\n",
    "        # FYI: split method does apply SVD by default\n",
    "        # there are variations of svd that can be inspected \n",
    "        # for a performance boost\n",
    "        SD = A.split(['i'+str(index-1),'v'+str(index)], absorb='right')\n",
    "    else:\n",
    "        SD = A.split(['i'+str(index-1),'v'+str(index)], absorb='left')\n",
    "       \n",
    "    # SD.tensors[0] -> I_{index}\n",
    "    # SD.tensors[1] -> I_{index+1}\n",
    "    return SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c5df43-b596-428d-b326-52823fa0eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_epoch(mps, imgs, epochs, lr):\n",
    "    '''\n",
    "    Manages the sliding left and right.\n",
    "    From tensor 1 (the second), apply learning_step() sliding to the right\n",
    "    At tensor max-2, apply learning_step() sliding to the left back to tensor 1\n",
    "    '''\n",
    "    \n",
    "    # [1,2,...,780,781,780,...,2,1]\n",
    "    progress = tq.tqdm([i for i in range(1,len(mps.tensors)-2)] + [i for i in range(len(mps.tensors)-3,0,-1)], leave=True)\n",
    "        \n",
    "    # Firstly we slide right \n",
    "    going_right = True\n",
    "    for index in progress:\n",
    "        A = learning_step(mps,index,imgs,lr, going_right)\n",
    "        \n",
    "        mps.tensors[index].modify(data=np.transpose(A.tensors[0].data,(0,2,1)))\n",
    "        mps.tensors[index+1].modify(data=A.tensors[1].data)\n",
    "\n",
    "        #p0 = computepsi(mps,imgs[0])**2\n",
    "        progress.set_description('Left Index: {}'.format(index))\n",
    "        \n",
    "        if index == len(mps.tensors)-3 :\n",
    "            going_right = False\n",
    "            \n",
    "    # cha cha real smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0659f-aed0-4e3e-bee3-0991171e4914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
