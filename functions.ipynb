{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8562bae-7049-485e-8a4f-a80d234aa2e9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e99b1f-d534-495f-a9d5-9a7475aef6e5",
   "metadata": {},
   "source": [
    "Some functions that i wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189971dc-b444-49a8-9b73-41acbb4ef7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### IMPORTS ####\n",
    "#################\n",
    "\n",
    "# Arrays\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning stuff\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Images display and plots\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1842c546-1c87-4e5b-853b-db3f428063ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Wrapper for type checks.\n",
    "While defining a function, you can add the wrapper\n",
    "stating the expected types:\n",
    "> @arg_val(class_1, class_2, ...)\n",
    "> def function(a, b, ...): \n",
    "'''\n",
    "def arg_val(*args):\n",
    "    def wrapper(func):\n",
    "        def validating(*_args):\n",
    "            if any(type(_arg)!=arg for _arg, arg in zip(_args,args)):\n",
    "                raise TypeError('wrong type!')\n",
    "            return func(*_args)\n",
    "        return validating\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d2ec9f-a2e7-4e27-9e74-24bdfc4a296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@arg_val(int, int, float)\n",
    "def get_data(train_size = 1000, test_size = 100, grayscale_threshold = .5):\n",
    "    '''\n",
    "    Prepare the MNIST dataset for the training algorithm:\n",
    "     * Choose randomly a subset from the whole dataset\n",
    "     * Flatten each image to mirror the mps structure\n",
    "     * Normalize images from [0,255] to [0,1]\n",
    "     * Apply a threshold for each pixels so that each value \n",
    "       below that threshold are set to 0, the others get set to 1.\n",
    "       For this algorithm we will only deal to binary states {0,1}\n",
    "       instead of a range from 0 to 1    \n",
    "    '''\n",
    "    # Download all data\n",
    "    mnist = torchvision.datasets.MNIST('classifier_data', train=True, download=True,\n",
    "                                                  transform = transforms.Compose([transforms.ToTensor()]) )\n",
    "    \n",
    "    # Convert torch.tenor to numpy\n",
    "    npmnist = mnist.data.numpy()\n",
    "    \n",
    "    # Check of the type of the sizes\n",
    "    #if ((type(train_size) != int) or (type(test_size) != int)):\n",
    "    #    raise TypeError('train_size and test_size must be INT')\n",
    "    \n",
    "    # Check if the training_size and test_size requested are bigger than\n",
    "    # the MNIST whole size\n",
    "    if ( (train_size + test_size) > npmnist.shape[0] ):\n",
    "        raise ValueError('Subset too big')\n",
    "    \n",
    "    # Check of the positivity of sizes\n",
    "    if ( (train_size <= 0) or (test_size <= 0) ):\n",
    "        raise ValueError('Size of training set and test set cannot be negative')\n",
    "    \n",
    "    # Choose just a subset of the data\n",
    "    # Creating a mask by randomly sampling the indexes of the full dataset\n",
    "    subset_indexes = np.random.choice(np.arange(npmnist.shape[0]), size=(train_size + test_size), \n",
    "                                      replace=False, p=None)\n",
    "    \n",
    "    # Apply the mask\n",
    "    npmnist = npmnist[subset_indexes]\n",
    "    \n",
    "    # Flatten every image\n",
    "    npmnist = np.reshape(npmnist, (npmnist.shape[0], npmnist.shape[1]*npmnist.shape[2]))\n",
    "    \n",
    "    # Normalize the data from 0 - 255 to 0 - 1\n",
    "    npmnist = npmnist/npmnist.max()\n",
    "    \n",
    "    # As in the paper, we will only deal with {0,1} values, not a range\n",
    "    \n",
    "    if ((grayscale_threshold <= 0) or (grayscale_threshold >= 1)):\n",
    "        raise ValueError('grayscale_threshold must be in range ]0,1[')\n",
    "    \n",
    "    npmnist[npmnist > grayscale_threshold] = 1\n",
    "    npmnist[npmnist <= grayscale_threshold] = 0\n",
    "    \n",
    "    # Return training set and test set\n",
    "    return npmnist[:train_size], npmnist[train_size:]\n",
    "\n",
    "@arg_val(np.ndarray, bool, str)\n",
    "def plot_img(img_flat, flip_color = True, savefig = ''):\n",
    "    '''\n",
    "    Display the image from the flattened form\n",
    "    '''\n",
    "    # If the image is corrupted for partial reconstruction (pixels are set to -1)\n",
    "    if -1 in img_flat:\n",
    "        img_flat = np.copy(img_flat)\n",
    "        img_flat[img_flat == -1] = 0\n",
    "    \n",
    "    # Background white, strokes black\n",
    "    if flip_color:\n",
    "        plt.imshow(1-np.reshape(img_flat,(28,28)), cmap='gray')\n",
    "    # Background black, strokes white\n",
    "    else:\n",
    "        plt.imshow(np.reshape(img_flat,(28,28)), cmap='gray')\n",
    "        \n",
    "    plt.axis('off')\n",
    "    \n",
    "    if savefig != '':\n",
    "        # save the picture as svg in the location determined by savefig\n",
    "        plt.savefig(savefig, format='svg')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fe208f-e019-4718-b55f-6af2ef3d4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c308e2-b5da-4d51-8ee9-a30319b82824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2c0833-3b8d-4259-b095-4fa23c6c7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28169cc1-f139-4e58-a553-8ac7ab78db94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ef578f-6c8f-4505-90c0-77b1c283f23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADJ0lEQVR4nO3dwWrDMBQAwar4/39ZPRfSuuAo2rgzx5gE+bA8yMN4zDk/gJ7P3QcAHhMnRIkTosQJUeKEqOPkur9yYb3x6EOTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IerYfYAdxhiXvj/nfNJJ4GcmJ0SJE6LECVHihChxQpQ4IUqcEHXbPefVXebK337XPenZfb/rfVWZnBAlTogSJ0SJE6LECVHihChxQtRt95wrd25X95wrd7Dch8kJUeKEKHFClDghSpwQJU6IEidE3XbPuVJ5h7qS5zVfy+SEKHFClDghSpwQJU6IEidEiROi7DljVu8Sy3tUvjM5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUV4BeDNXX/G3+hWE/J3JCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDgh6th9AJ5rzvnr9THGi07CVSYnRIkTosQJUeKEKHFClDghSpwQZc/5z5ztQekwOSFKnBAlTogSJ0SJE6LECVHihKizPaeH/2ATkxOixAlR4oQocUKUOCFKnBD1BbDuI+C/7vkgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(train_set[1], True, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabba680-6a17-45b6-928d-fef818afa679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa9a902-c4e7-47c1-ad39-ea5f2ca25779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import quimb.tensor as qtn # Tensor Network library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d72f97-9618-48a8-88c9-6b719de50b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MPS network randomly initialized\n",
    "mps = qtn.MPS_rand_state(L=28*28, bond_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43529da-7ed8-48bc-9273-1332406ef0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_img(mps, img):\n",
    "    '''\n",
    "    Contract the MPS network with an image to compute its probability\n",
    "    P(img) = (<mps|img><img|mps>)/<mps|mps>\n",
    "    '''\n",
    "    if (len(mps.tensors) != img.shape[0]):\n",
    "        raise ValueError('Length of MPS and size of image do not match')\n",
    "    \n",
    "    # Compute denominator\n",
    "    Z = mps.H @ mps # Does it acknowledge canonicalization to speed computations?\n",
    "                    # TO DO: check documentation\n",
    "        \n",
    "    # Contract image with mps\n",
    "    P = 0\n",
    "    # From left to right...\n",
    "    for body in range(img.shape[0]):\n",
    "        # if pixel is 0:\n",
    "        if img[body] == 0:\n",
    "            state = [1,0]\n",
    "        # if pixel is 1:\n",
    "        elif img[body] == 1:\n",
    "            state = [0,1]\n",
    "        else:\n",
    "            raise ValueError('Found invalid pixel in image')\n",
    "        \n",
    "        if body == img.shape[0] - 1:\n",
    "            newtensor = np.einsum('i,ik', carried_value, mps.tensors[body].data)\n",
    "            P = np.einsum('i,i', state, newtensor)\n",
    "        elif body > 0:\n",
    "            newtensor = np.einsum('i,ikj', carried_value, mps.tensors[body].data)\n",
    "            carried_value = np.einsum('i,ik', state, newtensor)\n",
    "        else:\n",
    "            carried_value = np.einsum('i,ki', state, mps.tensors[body].data)\n",
    "        \n",
    "        P = (P*P)/Z\n",
    "        \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeadb420-0933-4547-9bbb-3d37ffecbf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab247c4-1f09-4f7e-954c-65b117918133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_removal_img(mnistimg, fraction = .5, axis = 0):\n",
    "    '''\n",
    "    Corrupt (with -1 values) a portion of an input image (from the test set)\n",
    "    to test if the algorithm can reconstruct it\n",
    "    '''\n",
    "    # Check type:\n",
    "    if [type(mnistimg), type(fraction), type(axis)] != [np.ndarray, float, int]:\n",
    "        raise TypeError('Input types not valid')\n",
    "    \n",
    "    # Check the shape of input image\n",
    "    if (mnistimg.shape[0] != 784):\n",
    "        raise TypeError('Input image shape does not match, need (784,)')\n",
    "    \n",
    "    # Axis can be either 0 (rowise deletion) or 1 (columnwise deletion)\n",
    "    if not(axis in [0,1]):\n",
    "        raise ValueError('Invalid axis [0,1]')\n",
    "    \n",
    "    # Fraction must be from 0 to 1\n",
    "    if (fraction < 0 or fraction > 1):\n",
    "        raise ValueError('Invalid value for fraction variable (in interval [0,1])')\n",
    "        \n",
    "    mnistimg_corr = np.copy(mnistimg)\n",
    "    mnistimg_corr = np.reshape(mnistimg_corr, (28,28))\n",
    "    \n",
    "    if axis == 0:\n",
    "        mnistimg_corr[int(28*(1-fraction)):,:] = -1\n",
    "    else:\n",
    "        mnistimg_corr[:,int(28*(1-fraction)):] = -1\n",
    "        \n",
    "    mnistimg_corr = np.reshape(mnistimg_corr, (784,))\n",
    "    \n",
    "    return mnistimg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88236d8-ecb1-4409-ae39-b5b4035177b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = partial_removal_img(test_set[0], fraction = .3, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999e1270-4e9d-43c6-b749-659c3105077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-03-14T12:35:38.123862</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 231.84 \n",
       "L 231.84 231.84 \n",
       "L 231.84 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p8cc477ad70)\">\n",
       "    <image height=\"218\" id=\"imageb3e0c89dd9\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAACoElEQVR4nO3dMQ4CMQwAQYLu/182PRQUHBt0zPSR3KxcWVkzMzfgq+67B4B/IDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQIHDsHoBXa62P3s/MSZNwFhsNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAg4B7tgt7ds7lX69loEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBA4dg/A+WZm9wg8sdEgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAj4tukH+Xbpemw0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CDzyXEbKrrGr3AAAAAElFTkSuQmCC\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p8cc477ad70\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a800c2-927f-438f-acf5-aff534c63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mps(Ldim = 28*28, bdim = 30, canonicalize = 1):\n",
    "    '''\n",
    "    Initialize the MPS tensor network\n",
    "    1. Create the MPS TN\n",
    "    2. Canonicalization\n",
    "    3. Renaming indexes\n",
    "    '''\n",
    "    # Create a simple MPS network randomly initialized\n",
    "    mps = qtn.MPS_rand_state(L=Ldim, bond_dim=bdim)\n",
    "    \n",
    "    # Canonicalize: use a canonicalize value out of range to skip it (such as -1)\n",
    "    if canonicalize in range(Ldim):\n",
    "        mps.canonize(canonicalize)\n",
    "        mps.show()\n",
    "        \n",
    "    # REINDEXING TENSORS FOR A EASIER DEVELOPING\n",
    "    # during initializations, the index will be named using the same notation of the \n",
    "    # Pan Zhang et al paper:\n",
    "    #  ___       ___                      ___\n",
    "    # |I0|--i0--|I1|--i1-... ...-i(N-1)--|IN|\n",
    "    #  |         |                        |\n",
    "    #  | v0      | v1                     | vN\n",
    "    #  V         V                        V\n",
    "    \n",
    "    # Reindexing the leftmost tensor\n",
    "    mps = mps.reindex({mps.tensors[0].inds[0]: 'i0', \n",
    "                       mps.tensors[0].inds[1]: 'v0'})\n",
    "    \n",
    "    # Reindexing the inner tensors through a cycle\n",
    "    for tensor in range(1,len(mps.tensors)-1):\n",
    "        mps = mps.reindex({mps.tensors[tensor].inds[0]: 'i'+str(tensor-1),\n",
    "                           mps.tensors[tensor].inds[1]: 'i'+str(tensor),\n",
    "                           mps.tensors[tensor].inds[2]: 'v'+str(tensor)})\n",
    "    \n",
    "    # Reindexing the last tensor\n",
    "    tensor = tensor + 1\n",
    "    mps = mps.reindex({mps.tensors[tensor].inds[0]: 'i'+str(tensor), \n",
    "                       mps.tensors[tensor].inds[1]: 'v'+str(tensor)})  \n",
    "    \n",
    "    return mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db50fa1-3e86-42b2-b7ee-e75e30a3d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 8 4 2 \n",
      ">─●─<─<─<\n",
      "│ │ │ │ │\n"
     ]
    }
   ],
   "source": [
    "mps = initialize_mps(Ldim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a501bb46-7f23-4d50-ba12-af998ccedaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=(2, 2), inds=('i0', 'v0'), tags=oset(['I0'])),\n",
       " Tensor(shape=(2, 8, 2), inds=('i0', 'i1', 'v1'), tags=oset(['I1'])),\n",
       " Tensor(shape=(8, 4, 2), inds=('i1', 'i2', 'v2'), tags=oset(['I2'])),\n",
       " Tensor(shape=(4, 2, 2), inds=('i2', 'i4', 'v3'), tags=oset(['I3'])),\n",
       " Tensor(shape=(2, 2), inds=('i4', 'v4'), tags=oset(['I4'])))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443abba-4337-4e25-b778-58b29eda2f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
