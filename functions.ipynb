{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8562bae-7049-485e-8a4f-a80d234aa2e9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e99b1f-d534-495f-a9d5-9a7475aef6e5",
   "metadata": {},
   "source": [
    "Some functions that i wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189971dc-b444-49a8-9b73-41acbb4ef7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### IMPORTS ####\n",
    "#################\n",
    "\n",
    "# Arrays\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning stuff\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Images display and plots\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1842c546-1c87-4e5b-853b-db3f428063ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Wrapper for type checks.\n",
    "While defining a function, you can add the wrapper\n",
    "stating the expected types:\n",
    "> @arg_val(class_1, class_2, ...)\n",
    "> def function(a, b, ...): \n",
    "'''\n",
    "def arg_val(*args):\n",
    "    def wrapper(func):\n",
    "        def validating(*_args):\n",
    "            if any(type(_arg)!=arg for _arg, arg in zip(_args,args)):\n",
    "                raise TypeError('wrong type!')\n",
    "            return func(*_args)\n",
    "        return validating\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d2ec9f-a2e7-4e27-9e74-24bdfc4a296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@arg_val(int, int, float)\n",
    "def get_data(train_size = 1000, test_size = 100, grayscale_threshold = .5):\n",
    "    '''\n",
    "    Prepare the MNIST dataset for the training algorithm:\n",
    "     * Choose randomly a subset from the whole dataset\n",
    "     * Flatten each image to mirror the mps structure\n",
    "     * Normalize images from [0,255] to [0,1]\n",
    "     * Apply a threshold for each pixels so that each value \n",
    "       below that threshold are set to 0, the others get set to 1.\n",
    "       For this algorithm we will only deal to binary states {0,1}\n",
    "       instead of a range from 0 to 1    \n",
    "    '''\n",
    "    # Download all data\n",
    "    mnist = torchvision.datasets.MNIST('classifier_data', train=True, download=True,\n",
    "                                                  transform = transforms.Compose([transforms.ToTensor()]) )\n",
    "    \n",
    "    # Convert torch.tenor to numpy\n",
    "    npmnist = mnist.data.numpy()\n",
    "    \n",
    "    # Check of the type of the sizes\n",
    "    #if ((type(train_size) != int) or (type(test_size) != int)):\n",
    "    #    raise TypeError('train_size and test_size must be INT')\n",
    "    \n",
    "    # Check if the training_size and test_size requested are bigger than\n",
    "    # the MNIST whole size\n",
    "    if ( (train_size + test_size) > npmnist.shape[0] ):\n",
    "        raise ValueError('Subset too big')\n",
    "    \n",
    "    # Check of the positivity of sizes\n",
    "    if ( (train_size <= 0) or (test_size <= 0) ):\n",
    "        raise ValueError('Size of training set and test set cannot be negative')\n",
    "    \n",
    "    # Choose just a subset of the data\n",
    "    # Creating a mask by randomly sampling the indexes of the full dataset\n",
    "    subset_indexes = np.random.choice(np.arange(npmnist.shape[0]), size=(train_size + test_size), \n",
    "                                      replace=False, p=None)\n",
    "    \n",
    "    # Apply the mask\n",
    "    npmnist = npmnist[subset_indexes]\n",
    "    \n",
    "    # Flatten every image\n",
    "    npmnist = np.reshape(npmnist, (npmnist.shape[0], npmnist.shape[1]*npmnist.shape[2]))\n",
    "    \n",
    "    # Normalize the data from 0 - 255 to 0 - 1\n",
    "    npmnist = npmnist/npmnist.max()\n",
    "    \n",
    "    # As in the paper, we will only deal with {0,1} values, not a range\n",
    "    \n",
    "    if ((grayscale_threshold <= 0) or (grayscale_threshold >= 1)):\n",
    "        raise ValueError('grayscale_threshold must be in range ]0,1[')\n",
    "    \n",
    "    npmnist[npmnist > grayscale_threshold] = 1\n",
    "    npmnist[npmnist <= grayscale_threshold] = 0\n",
    "    \n",
    "    # Return training set and test set\n",
    "    return npmnist[:train_size], npmnist[train_size:]\n",
    "\n",
    "@arg_val(np.ndarray, bool, str)\n",
    "def plot_img(img_flat, flip_color = True, savefig = ''):\n",
    "    '''\n",
    "    Display the image from the flattened form\n",
    "    '''\n",
    "    # If the image is corrupted for partial reconstruction (pixels are set to -1)\n",
    "    if -1 in img_flat:\n",
    "        img_flat = np.copy(img_flat)\n",
    "        img_flat[img_flat == -1] = 0\n",
    "    \n",
    "    # Background white, strokes black\n",
    "    if flip_color:\n",
    "        plt.imshow(1-np.reshape(img_flat,(28,28)), cmap='gray')\n",
    "    # Background black, strokes white\n",
    "    else:\n",
    "        plt.imshow(np.reshape(img_flat,(28,28)), cmap='gray')\n",
    "        \n",
    "    plt.axis('off')\n",
    "    \n",
    "    if savefig != '':\n",
    "        # save the picture as svg in the location determined by savefig\n",
    "        plt.savefig(savefig, format='svg')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fe208f-e019-4718-b55f-6af2ef3d4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34fea7b0a9a49188bb3acf5cd73f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting classifier_data/MNIST/raw/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bcf80fefd747128a9025f8be358641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c650446010fd4915a03e5f534dd29e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bd123f0d504f8ab20bac71e11a2669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c308e2-b5da-4d51-8ee9-a30319b82824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2c0833-3b8d-4259-b095-4fa23c6c7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28169cc1-f139-4e58-a553-8ac7ab78db94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ef578f-6c8f-4505-90c0-77b1c283f23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADUElEQVR4nO3dS07DQBRFQYyy/y2bMcLE+Tjuk3TVECaZHD2Jqw7Luq5fQM/36A8AbBMnRIkTosQJUeKEqMvO7/0pF15v2fqhywlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiLqM/gDcZ1mW0R/hX+u6jv4IH8XlhChxQpQ4IUqcECVOiBInRIkTouycHObZDdZO+pvLCVHihChxQpQ4IUqcECVOiDKlkHFtiplxZnE5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQo7zkns/cusvwvBmfjckKUOCFKnBAlTogSJ0SJE6LECVF2TjJm/G7aa1xOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUJ2OTGfnVl56E3cflhChxQpQ4IUqcECVOiBInRIkTouycHMaOeSyXE6LECVHihChxQpQ4IUqcECVOiLJzxox8b7nHjnkulxOixAlR4oQocUKUOCFKnBAlToiycw5gy+QWLidEiROixAlR4oQocUKUOCFKnBBl53yB8o7J+3A5IUqcECVOiBInRIkTosQJUaaUB5hKOIPLCVHihChxQpQ4IUqcECVOiBInRNk5Y/a+mtLGOg+XE6LECVHihChxQpQ4IUqcECVOiLJzDuDf7HELlxOixAlR4oQocUKUOCFKnBAlToiyc27wZpIClxOixAlR4oQocUKUOCFKnBBlSol59Yzjudr7cDkhSpwQJU6IEidEiROixAlR4oQoO+cAnqRxC5cTosQJUeKEKHFClDghSpwQJU6IsnNu2HvzWN4pvdf8HC4nRIkTosQJUeKEKHFClDghSpwQZed8M3bMebicECVOiBInRIkTosQJUeKEKHFClJ3zAbZGzuByQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6L2vhpzOeVTAH+4nBAlTogSJ0SJE6LECVHihKgf/4018M6N9U8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(train_set[1], True, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabba680-6a17-45b6-928d-fef818afa679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa9a902-c4e7-47c1-ad39-ea5f2ca25779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import quimb.tensor as qtn # Tensor Network library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d72f97-9618-48a8-88c9-6b719de50b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MPS network randomly initialized\n",
    "mps = qtn.MPS_rand_state(L=28*28, bond_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43529da-7ed8-48bc-9273-1332406ef0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_img(mps, img):\n",
    "    '''\n",
    "    Contract the MPS network with an image to compute its probability\n",
    "    P(img) = (<mps|img><img|mps>)/<mps|mps>\n",
    "    '''\n",
    "    if (len(mps.tensors) != img.shape[0]):\n",
    "        raise ValueError('Length of MPS and size of image do not match')\n",
    "    \n",
    "    # Compute denominator\n",
    "    Z = mps.H @ mps # Does it acknowledge canonicalization to speed computations?\n",
    "                    # TO DO: check documentation\n",
    "        \n",
    "    # Contract image with mps\n",
    "    P = 0\n",
    "    # From left to right...\n",
    "    for body in range(img.shape[0]):\n",
    "        # if pixel is 0:\n",
    "        if img[body] == 0:\n",
    "            state = [1,0]\n",
    "        # if pixel is 1:\n",
    "        elif img[body] == 1:\n",
    "            state = [0,1]\n",
    "        else:\n",
    "            raise ValueError('Found invalid pixel in image')\n",
    "        \n",
    "        if body == img.shape[0] - 1:\n",
    "            newtensor = np.einsum('i,ik', carried_value, mps.tensors[body].data)\n",
    "            P = np.einsum('i,i', state, newtensor)\n",
    "        elif body > 0:\n",
    "            newtensor = np.einsum('i,ikj', carried_value, mps.tensors[body].data)\n",
    "            carried_value = np.einsum('i,ik', state, newtensor)\n",
    "        else:\n",
    "            carried_value = np.einsum('i,ki', state, mps.tensors[body].data)\n",
    "        \n",
    "        P = (P*P)/Z\n",
    "        \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeadb420-0933-4547-9bbb-3d37ffecbf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab247c4-1f09-4f7e-954c-65b117918133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_removal_img(mnistimg, fraction = .5, axis = 0):\n",
    "    '''\n",
    "    Corrupt (with -1 values) a portion of an input image (from the test set)\n",
    "    to test if the algorithm can reconstruct it\n",
    "    '''\n",
    "    # Check type:\n",
    "    if [type(mnistimg), type(fraction), type(axis)] != [np.ndarray, float, int]:\n",
    "        raise TypeError('Input types not valid')\n",
    "    \n",
    "    # Check the shape of input image\n",
    "    if (mnistimg.shape[0] != 784):\n",
    "        raise TypeError('Input image shape does not match, need (784,)')\n",
    "    \n",
    "    # Axis can be either 0 (rowise deletion) or 1 (columnwise deletion)\n",
    "    if not(axis in [0,1]):\n",
    "        raise ValueError('Invalid axis [0,1]')\n",
    "    \n",
    "    # Fraction must be from 0 to 1\n",
    "    if (fraction < 0 or fraction > 1):\n",
    "        raise ValueError('Invalid value for fraction variable (in interval [0,1])')\n",
    "        \n",
    "    mnistimg_corr = np.copy(mnistimg)\n",
    "    mnistimg_corr = np.reshape(mnistimg_corr, (28,28))\n",
    "    \n",
    "    if axis == 0:\n",
    "        mnistimg_corr[int(28*(1-fraction)):,:] = -1\n",
    "    else:\n",
    "        mnistimg_corr[:,int(28*(1-fraction)):] = -1\n",
    "        \n",
    "    mnistimg_corr = np.reshape(mnistimg_corr, (784,))\n",
    "    \n",
    "    return mnistimg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88236d8-ecb1-4409-ae39-b5b4035177b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = partial_removal_img(test_set[0], fraction = .3, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999e1270-4e9d-43c6-b749-659c3105077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-03-14T19:24:46.448283</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 231.84 \n",
       "L 231.84 231.84 \n",
       "L 231.84 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p78b80407bd)\">\n",
       "    <image height=\"218\" id=\"image53f8b9fbb6\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAADB0lEQVR4nO3dYUrDQBhFUSvuf8vjb0ENrc7NJHPOAkopXD7II/oYY4w3YKr3s78A7EBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUHg4+wvcJbH4zH188cYUz9/VTN/1yv/pi4aBIQGAaFBQGgQEBoEhAYBoUFg2x3taJM52oOuvOn8xez98a5cNAgIDQJCg4DQICA0CAgNAkKDwLY72pFddzLmcNEgIDQICA0CQoOA0CAgNAh4vM8XZ74Gc+dJxUWDgNAgIDQICA0CQoOA0CAgNAjY0W5o5T8Jd+et7DcuGgSEBgGhQUBoEBAaBIQGAaFBwI62oJV3sCO77mRHXDQICA0CQoOA0CAgNAgIDQJCg8C2O9qVt6qZ7GBzuGgQEBoEhAYBoUFAaBAQGgSEBoFtd7Q7s4Wtx0WDgNAgIDQICA0CQoOA0CDg8f6CPJ6/HxcNAkKDgNAgIDQICA0CQoOA0CBgRzuBnWw/LhoEhAYBoUFAaBAQGgSEBgGhQcCOdoIr/8soG+BrXDQICA0CQoOA0CAgNAgIDQJCg4AdjaccbYB2tu+5aBAQGgSEBgGhQUBoEBAaBIQGgW13tKO958rvjLEeFw0CQoOA0CAgNAgIDQJCg8C2j/dX5lWT+3HRICA0CAgNAkKDgNAgIDQICA0CdrQfeI2G/+SiQUBoEBAaBIQGAaFBQGgQEBoE7Ggv8s4Yz3DRICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQIfALK0ju+oaTbPAAAAABJRU5ErkJggg==\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p78b80407bd\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a800c2-927f-438f-acf5-aff534c63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mps(Ldim = 28*28, bdim = 30, canonicalize = 1):\n",
    "    '''\n",
    "    Initialize the MPS tensor network\n",
    "    1. Create the MPS TN\n",
    "    2. Canonicalization\n",
    "    3. Renaming indexes\n",
    "    '''\n",
    "    # Create a simple MPS network randomly initialized\n",
    "    mps = qtn.MPS_rand_state(L=Ldim, bond_dim=bdim)\n",
    "    \n",
    "    # Canonicalize: use a canonicalize value out of range to skip it (such as -1)\n",
    "    if canonicalize in range(Ldim):\n",
    "        mps.canonize(canonicalize)\n",
    "        \n",
    "    # REINDEXING TENSORS FOR A EASIER DEVELOPING\n",
    "    # during initializations, the index will be named using the same notation of the \n",
    "    # Pan Zhang et al paper:\n",
    "    #  ___       ___                      ___\n",
    "    # |I0|--i0--|I1|--i1-... ...-i(N-1)--|IN|\n",
    "    #  |         |                        |\n",
    "    #  | v0      | v1                     | vN\n",
    "    #  V         V                        V\n",
    "    \n",
    "    # Reindexing the leftmost tensor\n",
    "    mps = mps.reindex({mps.tensors[0].inds[0]: 'i0', \n",
    "                       mps.tensors[0].inds[1]: 'v0'})\n",
    "    \n",
    "    # Reindexing the inner tensors through a cycle\n",
    "    for tensor in range(1,len(mps.tensors)-1):\n",
    "        mps = mps.reindex({mps.tensors[tensor].inds[0]: 'i'+str(tensor-1),\n",
    "                           mps.tensors[tensor].inds[1]: 'i'+str(tensor),\n",
    "                           mps.tensors[tensor].inds[2]: 'v'+str(tensor)})\n",
    "    \n",
    "    # Reindexing the last tensor\n",
    "    tensor = tensor + 1\n",
    "    mps = mps.reindex({mps.tensors[tensor].inds[0]: 'i'+str(tensor), \n",
    "                       mps.tensors[tensor].inds[1]: 'v'+str(tensor)})  \n",
    "    \n",
    "    return mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db50fa1-3e86-42b2-b7ee-e75e30a3d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = initialize_mps(Ldim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a501bb46-7f23-4d50-ba12-af998ccedaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=(2, 2), inds=('i0', 'v0'), tags=oset(['I0'])),\n",
       " Tensor(shape=(2, 8, 2), inds=('i0', 'i1', 'v1'), tags=oset(['I1'])),\n",
       " Tensor(shape=(8, 4, 2), inds=('i1', 'i2', 'v2'), tags=oset(['I2'])),\n",
       " Tensor(shape=(4, 2, 2), inds=('i2', 'i4', 'v3'), tags=oset(['I3'])),\n",
       " Tensor(shape=(2, 2), inds=('i4', 'v4'), tags=oset(['I4'])))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b443abba-4337-4e25-b778-58b29eda2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quimb_transform_img2state(img):\n",
    "    '''\n",
    "    Trasform an image to a tensor network to fully manipulate\n",
    "    it using quimb, may be very slow, use it for checks\n",
    "    '''\n",
    "    \n",
    "    # Initialize empty tensor\n",
    "    img_TN = qtn.Tensor()\n",
    "    for k, pixel in enumerate(img):\n",
    "        if pixel == 0: # if pixel is 0, we want to have a tensor with data [0,1]\n",
    "            img_TN = img_TN &  qtn.Tensor(data=[0,1], inds=['v'+str(k)], )\n",
    "            \n",
    "        else: # if pixel is 1, we want to have a tensor with data [1,0]\n",
    "            img_TN = img_TN &  qtn.Tensor(data=[1,0], inds=['v'+str(k)], )\n",
    "     \n",
    "    # |  | 781 |\n",
    "    # O  O ... O\n",
    "    return img_TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f6cbcdb-9de6-4355-af95-419de61659a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computepsi(mps, img):\n",
    "    '''\n",
    "    Contract the MPS with the states (pixels) of a binary{0,1} image\n",
    "    \n",
    "    PSI:    O-...-O-O-O-...-O\n",
    "            |     | | |     |\n",
    "            |     | | |     |\n",
    "    IMAGE:  O     O O O     O\n",
    "    \n",
    "    Images state are created the following way:\n",
    "    if pixel is 0 -> state = [0,1]\n",
    "    if pixel is 1 -> state = [1,0]\n",
    "    '''\n",
    "    \n",
    "    # Left most tensor\n",
    "    #          O--\n",
    "    # Compute  |  => O--\n",
    "    #          O\n",
    "    if img[0] == 0:\n",
    "        contraction = np.einsum('a,ba',[0,1], mps.tensors[0].data)\n",
    "    else:\n",
    "        contraction = np.einsum('a,ba',[1,0], mps.tensors[0].data)\n",
    "        \n",
    "    # Remove the first and last pixels because in the MPS\n",
    "    # They need to be treated differently\n",
    "    for k, pixel in enumerate(img[1:-1]):\n",
    "        #  \n",
    "        # Compute  O--O--  => O--\n",
    "        #             |       |\n",
    "        contraction = np.einsum('a,abc',contraction, mps.tensors[k+1].data)\n",
    "        \n",
    "        #          O--\n",
    "        # Compute  |  => O--\n",
    "        #          O        \n",
    "        if pixel == 0:\n",
    "            contraction = np.einsum('a,ba', [0,1], contraction)\n",
    "        else:\n",
    "            contraction = np.einsum('a,ba', [1,0], contraction)\n",
    "    \n",
    "    #          \n",
    "    # Compute  O--O  => O\n",
    "    #             |     |\n",
    "    contraction = np.einsum('a,ab',contraction, mps.tensors[-1].data)\n",
    "    \n",
    "    #          O\n",
    "    # Compute  |  => O (SCALAR)\n",
    "    #          O     \n",
    "    if img[-1] == 0:\n",
    "        contraction = np.einsum('a,a', [0,1], contraction)\n",
    "    else:\n",
    "        contraction = np.einsum('a,a', [1,0], contraction)\n",
    "    \n",
    "    return contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36765976-6972-460d-bab0-adcf2cf34331",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = initialize_mps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e69d4e2d-f36c-4d47-8024-957ec44f1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 0 ns, total: 17.2 ms\n",
      "Wall time: 15.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "fast_psi = computepsi(mps, train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "911c1264-e92c-42a5-9b7a-b0c1a348f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.49 s, sys: 6.99 ms, total: 2.49 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "slow_psi = quimb_transform_img2state(train_set[0]) @ mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4c71d7d-ba5d-4ddb-b57b-7dcb3f578568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.792058477343438e-251"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_psi**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cda11c23-fce1-45a1-a83d-fe61e1aa72ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.792058477343171e-251"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_psi**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
